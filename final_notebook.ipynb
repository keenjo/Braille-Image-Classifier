{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fae59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all of the necessary packages\n",
    "import os\n",
    "import copy\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from skimage import io, transform\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sacred import Experiment\n",
    "from sacred.observers import FileStorageObserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76435dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    # Dataset class for our images\n",
    "\n",
    "    def __init__(self, data_dir=\"Braille Dataset/\"):\n",
    "        # Initialization of data directory and list of all of the paths to each image in the data\n",
    "        self.data_dir = data_dir\n",
    "        self.image_path_list = sorted(self._find_files(data_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Function to get the length of the dataset\n",
    "        '''\n",
    "        return len(self.image_path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Function to be able to select images and corresponding labels from the dataset\n",
    "        '''\n",
    "        # Convert string labels to integers\n",
    "        labels = []\n",
    "        for path in self.image_path_list:\n",
    "            label = path.replace(self.data_dir + 'sorted_data/', '')[0]\n",
    "            labels.append(label)\n",
    "        # Creating a set of all possible labels for our task\n",
    "        labels = sorted(list(set(labels)))\n",
    "        # Creating a dictionary that maps labels to integers\n",
    "        labels2tensor = {label: labels.index(label) for label in labels}\n",
    "\n",
    "        image_path_ex = self.image_path_list[index]\n",
    "        label_ex = image_path_ex.replace(self.data_dir + 'sorted_data/', '')[0]\n",
    "        # Load image\n",
    "        image_ex = io.imread(image_path_ex)\n",
    "        # Normalize image (make values between 0 and 1)\n",
    "        image_ex = image_ex / np.max(image_ex)\n",
    "        # Function to transform the images to float32 tensors\n",
    "        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                    torchvision.transforms.ConvertImageDtype(dtype=torch.float32)])\n",
    "        # Transform image and get label's corresponding integer\n",
    "        image_ex = transform(image_ex)\n",
    "        label_ex = labels2tensor[label_ex]\n",
    "\n",
    "        return image_ex, label_ex\n",
    "\n",
    "    def prep_data(self, data_dir):\n",
    "        '''\n",
    "        Function to organize the letter images into one directory for each letter\n",
    "        - this function assumes the dataset was downloaded and unzipped in the same directory as the python scripts\n",
    "        '''\n",
    "        os.makedirs(f'{data_dir}/sorted_data/', exist_ok=True)  # Creates a sorted_data directory within Braille Dataset directory\n",
    "        for root, dirs, files in os.walk(f'{data_dir}/Braille Dataset'):\n",
    "            for file in tqdm(sorted(files)):\n",
    "                if file.endswith('.jpg'):\n",
    "                    os.makedirs(f'{data_dir}/sorted_data/{file[0]}/',\n",
    "                                exist_ok=True)  # Adds a directory for each letter of the alphabet\n",
    "                    copyfile(f'{root}/{file}',\n",
    "                             f'{data_dir}/sorted_data/{file[0]}/{file}')  # Adds each letter image to it's corresponding directory\n",
    "\n",
    "    def _find_files(self, directory):\n",
    "        '''\n",
    "        Function to get all files in data directory\n",
    "        '''\n",
    "        image_path_list = []\n",
    "        sorted_dir = os.path.join(directory, \"sorted_data\")\n",
    "        if not os.path.isdir(sorted_dir):\n",
    "            print(\"Processing the data.\")\n",
    "            self.prep_data(self.data_dir)\n",
    "        for letter in string.ascii_lowercase:\n",
    "            curr_dir = os.path.join(sorted_dir, letter)\n",
    "            image_path_list += [os.path.join(curr_dir, f) for f in os.listdir(curr_dir)]\n",
    "        return image_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce4aba",
   "metadata": {},
   "source": [
    "## Explanation of Model Architecture\n",
    "\n",
    "Since we were dealing with convolutional classifier for this task we started out with a basic model using 2 convolutional layers to process the image and 1 linear layer for classification. Then, to improve our model we started adding more convolutional and linear layers, settling on 3 convolutional and 3 linear layers. Next, we expoerimented with other functionalities within the convolutional layers such as Batch Normalization, Max Pooling and Dropout. After our experiments we found that Max Pooling for all convolutional layers performed best, while Batch Normalization and Dropout did not improve performance for this case. Lastly, we wanted to find the perfect number of channels, kernel size, padding, and max pooling size for the convolutional layers and number of dimensions for the linear layers. We found that performance improved as we increased the size of the layers, but only up to a certain point and performance also seemed to be best when theses values were all factors of 4. For all of the other values within the convolutional layers, we experimented with many values, but our intial ones of 5 and 3 for kernel size, 2 for max pooling kernel size, and 2 for padding performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3c48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassif(nn.Module):\n",
    "    \"\"\"Convolutional neural network classifier for Braille letter images\"\"\"\n",
    "    def __init__(self, num_channels1=16, num_channels2=32, num_channels3=64,\n",
    "                 num_lin_channels1=128, num_lin_channels2=64, num_classes=26):\n",
    "        super(CNNClassif, self).__init__()\n",
    "        # Convolutional channel values\n",
    "        self.num_channels1 = num_channels1\n",
    "        self.num_channels2 = num_channels2\n",
    "        self.num_channels3 = num_channels3\n",
    "        # Linear dimension values\n",
    "        self.num_lin_channels1 = num_lin_channels1\n",
    "        self.num_lin_channels2 = num_lin_channels2\n",
    "        # Number of classes\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Convolutional layer 1 with ReLU actication and Max Pooling\n",
    "        self.cnn_layer1 = nn.Sequential(nn.Conv2d(3, num_channels1, kernel_size=5, padding=2), \n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(kernel_size=2))\n",
    "        # Convolutional layer 2 with ReLU actication and Max Pooling\n",
    "        self.cnn_layer2 = nn.Sequential(nn.Conv2d(num_channels1, num_channels2, kernel_size=5, padding=2), \n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(kernel_size=2))\n",
    "        # Convolutional layer 3 with ReLU actication and Max Pooling\n",
    "        self.cnn_layer3 = nn.Sequential(nn.Conv2d(num_channels2, num_channels3, kernel_size=3, padding=2), \n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(kernel_size=2))\n",
    "        # Linear layer 1 with ReLU activation\n",
    "        self.linear_layer1 = nn.Sequential(nn.Linear(num_channels3*4*4, num_lin_channels1), nn.ReLU())\n",
    "        # Linear layer 2 with ReLU activation\n",
    "        self.linear_layer2 = nn.Sequential(nn.Linear(num_lin_channels1, num_lin_channels2), nn.ReLU())\n",
    "        # Linear layer 3 with ReLU activation\n",
    "        self.linear_layer3 = nn.Sequential(nn.Linear(num_lin_channels2, num_classes), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the model to pass the batches through each layer of the model\n",
    "        '''\n",
    "        w = self.cnn_layer1(x)\n",
    "        y = self.cnn_layer2(w)\n",
    "        z = self.cnn_layer3(y)\n",
    "        z2 = z.reshape(z.shape[0], -1)\n",
    "        lin1 = self.linear_layer1(z2)\n",
    "        lin2 = self.linear_layer2(lin1)\n",
    "        out = self.linear_layer3(lin2)\n",
    "        return out \n",
    "\n",
    "def init_weights(m):\n",
    "    '''\n",
    "    Function to initialize the weights of the model\n",
    "    '''\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c8cd4",
   "metadata": {},
   "source": [
    "We wrapped our code in a sacred experiment in order to test different (hyper)parameters.\n",
    "The results are saved locally in `runs/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a sacred experiment\n",
    "ex = Experiment('braille_cnn', interactive=True)\n",
    "ex.observers.append(FileStorageObserver('runs'))\n",
    "\n",
    "@ex.config\n",
    "def config():\n",
    "    \"\"\"Configuration of the Braille Image Classifier experiment.\"\"\"\n",
    "    # Random seed value\n",
    "    seed = 0\n",
    "    # Number of images per batch\n",
    "    batch_size = 8\n",
    "    # Maximum number of training epochs\n",
    "    num_epochs = 50\n",
    "    # Loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Learning rate\n",
    "    learning_rate = 0.01\n",
    "    # Patience value: maximum number of epochs to continue training without improvement of validation accuracy\n",
    "    # - Used to implement early stopping\n",
    "    patience = 10\n",
    "    # Number of convolutional channels for layers 1-3\n",
    "    num_channels1 = 32\n",
    "    num_channels2 = 64\n",
    "    num_channels3 = 128\n",
    "    # Number of dimensions for the last 2 linear layers\n",
    "    num_lin_channels1 = 128\n",
    "    num_lin_channels2 = 64\n",
    "    \n",
    "    \n",
    "@ex.capture\n",
    "def training_cnn_classifier(model, train_dataloader, val_dataloader, num_epochs, loss_fn,\n",
    "                            learning_rate, patience, verbose=True):\n",
    "    '''\n",
    "    Function to train the CNN classifier\n",
    "    - Parameters:\n",
    "        - model: instantiation of the CNN\n",
    "        - train_dataloader: training dataloader\n",
    "        - val_dataloader: validation dataloader\n",
    "        - num_epochs: maximum number of epochs\n",
    "        - loss_fn: loss function\n",
    "        - learning_rate: learning rate\n",
    "        - patience: patience value (maximum number of epochs to continue training without validation accuracy improvement before early stoppage)\n",
    "    - Returns:\n",
    "        - model_tr: our model with the adjusted parameters after training\n",
    "        - loss_all_epochs: A list containing the loss value for each epoch\n",
    "    '''\n",
    "    \n",
    "    # Create a deepcopy of the model and commence training\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    model_tr.train()\n",
    "    \n",
    "    # Define the optimizer to use in the model\n",
    "    optimizer = torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize the list that will contain all loss values and set no_improve early stopping variable to 0\n",
    "    loss_all_epochs = []\n",
    "    no_improve = 0  # value to track for how many epochs validation accuracy is not improving\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set loss for the current epoch to 0\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate through all batches for a given epoch\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Run the forward pass of the model and compute the loss based on the obtained y_pred predication and correct labels\n",
    "            y_pred = model_tr.forward(images)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            \n",
    "            # Run backward pass of the model to adjust the weights using our defined optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the epoch loss with the loss from the currect batch\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # Append the average loss over the entire epoch to the loss_all_epochs list\n",
    "        loss_all_epochs.append(loss_current_epoch / (batch_index + 1))\n",
    "        # Run the model on the validation data\n",
    "        val_accuracy = eval_cnn_classifier(model_tr, eval_dataloader=val_dataloader)\n",
    "        \n",
    "        # Early stopping implementation to check if validation accuracy is improving\n",
    "        if epoch == 0:\n",
    "            best_acc = val_accuracy\n",
    "        elif val_accuracy > best_acc:\n",
    "            best_acc = val_accuracy\n",
    "            torch.save(model_tr.state_dict(), 'test_model.pt')\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss_current_epoch/(batch_index + 1):.4f}')\n",
    "            print(f'-----> Validation Accuracy: {val_accuracy:.3f}%')\n",
    "            ex.log_scalar('loss', loss_current_epoch, step=epoch+1)\n",
    "        \n",
    "        # When the number of epochs without validation accuracy improvement is greater than our patience value we end training\n",
    "        if no_improve >= patience:\n",
    "            break\n",
    "        \n",
    "    return model_tr, loss_all_epochs\n",
    "\n",
    "\n",
    "def eval_cnn_classifier(model, eval_dataloader):\n",
    "    '''\n",
    "    Function to run evaluation on the classifier\n",
    "    - Parameters:\n",
    "        - model: model\n",
    "        - eval_dataloader: evaluation dataloader; could be validation or test data\n",
    "    - Returns:\n",
    "        - accuracy: accuracy of the model on the evaluation data\n",
    "    '''\n",
    "    \n",
    "    # Set model into evaluation mode, since we won't be adjusting any weights here\n",
    "    model.eval() \n",
    "\n",
    "    # While disabling the gradient calculation (since we are not training), run the model on the eval_dataloader\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in eval_dataloader:\n",
    "            y_predicted = model(images)\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@ex.main\n",
    "def run(seed, batch_size, num_epochs,\n",
    "        num_channels1, num_channels2, num_channels3,\n",
    "        num_lin_channels1, num_lin_channels2):\n",
    "    '''\n",
    "    Function to run all of our data preprocessing, training, and evaluation\n",
    "    - Parameters:\n",
    "        - seed: Random seed\n",
    "        - batch_size: batch size\n",
    "        - num_epochs: maximum number of epochs\n",
    "        - num_channels1: number of channels for 1st convolutional layer\n",
    "        - num_channels2: number of channels for the 2nd convolutional layer\n",
    "        - num_channels3: number of channels for the 3rd convolutional layer\n",
    "        - num_lin_channels1: number of dimensions for the second linear layer \n",
    "        - num_lin)channels2: number of dimensions for the third and final linear layer\n",
    "    '''\n",
    "\n",
    "    # Instantiating the dataset\n",
    "    dataset = ImageDataset()\n",
    "    # Splitting the dataset\n",
    "    split_data = random_split(dataset, [1248, 156, 156], generator=torch.Generator().manual_seed(seed))\n",
    "    train_data, val_data, test_data = split_data\n",
    "\n",
    "    # Creating the dataloaders for each split of our dataset\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Printing the number of classes, batch shape and number of batches for training\n",
    "    num_classes = len(list(set([datapoint[1] for datapoint in train_data])))\n",
    "    print(\"Number of classes: \", num_classes)\n",
    "    batch_data, batch_name =  next(iter(train_dataloader))\n",
    "    print(f'Batch shape [batch_size, image_shape]: {batch_data.shape}')\n",
    "    print('Number of batches:', len(train_dataloader))\n",
    "\n",
    "    print(\"== Initializing model...\")\n",
    "    model = CNNClassif(num_channels1, num_channels2, num_channels3,\n",
    "                       num_lin_channels1, num_lin_channels2, num_classes)\n",
    "    torch.manual_seed(seed)\n",
    "    model.apply(init_weights)\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    ex.log_scalar('number_of_params', num_params)\n",
    "    print(model)\n",
    "\n",
    "    print(\"== Training...\")\n",
    "    model, loss_total = training_cnn_classifier(model, train_dataloader, val_dataloader)\n",
    "    ex.add_artifact('test_model.pt')\n",
    "\n",
    "    plt.plot(loss_total)\n",
    "    plt.savefig('loss.png')\n",
    "    ex.add_artifact('loss.png')\n",
    "\n",
    "    print(\"== Evaluating...\")\n",
    "    # Instantiating and loading the best model checkpoint from training so we can run evaluation on it\n",
    "    model_eval = CNNClassif(num_channels1, num_channels2, num_channels3,\n",
    "                            num_lin_channels1, num_lin_channels2, num_classes)\n",
    "    model_eval.load_state_dict(torch.load('test_model.pt'))\n",
    "    accuracy = eval_cnn_classifier(model_eval, test_dataloader)\n",
    "    ex.log_scalar('accuracy', accuracy)\n",
    "    \n",
    "    return f'{accuracy:.3f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99deeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment\n",
    "# no arguments are passed as sacred provides them implicitly from ex.config\n",
    "ex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5bff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnenv",
   "language": "python",
   "name": "nnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
