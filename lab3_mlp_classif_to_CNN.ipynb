{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification with multilayer perceptron (MLP)\n",
    "\n",
    "In this lab we design and train our first MLP network, and we use it for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this lab, we use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset, which is a large image dataset of hand-written digits.\n",
    "\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/MNIST_database\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></a></center>\n",
    "\n",
    "Just like some other widely-used datasets, MNIST can be downloaded directly from Pytorch, and includes specific commands to create a `Dataset` object, thus you don't have to do it manually as we did in lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data repository\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Choose one (or several) transform(s) to preprocess the data\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Create a Dataset (you can download the data by setting 'download=True')\n",
    "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=False, transform=data_transforms)\n",
    "test_data = torchvision.datasets.MNIST(data_dir, train=False, download=False, transform=data_transforms)\n",
    "num_classes = len(train_data.classes)\n",
    "print('Number of classes in the dataset:', num_classes)\n",
    "\n",
    "# We are not going to work with the full dataset (which is very big), so we only keep small train and test subsets.\n",
    "train_data = Subset(train_data, torch.arange(400))\n",
    "test_data = Subset(test_data, torch.arange(50))\n",
    "print('Number of images in the train dataset', len(train_data))\n",
    "print('Number of images in the test dataset', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one data pair (one image and the corresponding label)\n",
    "image, label = train_data[0]\n",
    "print(image.shape)\n",
    "print('Image label=', label)\n",
    "\n",
    "# You should note that the size of the image is [1, 28, 28], which corresponds to [num_channels, height, width]\n",
    "# Indeed, MNIST images are in black and white so there is only 1 color channel.\n",
    "# To plot this image, we need to remove this channel dimension by using squeeze()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze(), cmap='gray_r')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - create two dataloaders (for the training and testing subsets) with a batch size of 8\n",
    "# - print the number of batches in the training subset\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print('Number of batches in the training subset: ', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and corresponding labels from the train dataloader\n",
    "batch_example = next(iter(train_dataloader))\n",
    "image_batch_example = batch_example[0]\n",
    "labels_batch_example = batch_example[1]\n",
    "\n",
    "# Print the size of the batch of images and labels\n",
    "print(image_batch_example.shape)\n",
    "print(labels_batch_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images in the batch (along with the corresponding label)\n",
    "plt.figure(figsize = (10,6))\n",
    "for ib in range(batch_size):\n",
    "    plt.subplot(batch_size // 4, 4, ib+1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Image label = ' + str(labels_batch_example[ib].item()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP network\n",
    "\n",
    "A typical MLP network is composed of several layers:\n",
    "\n",
    "- an *input* layer, which takes a batch of vectors and computes the first hidden representation.\n",
    "- one or several *hidden* layers.\n",
    "- an *output* layer, which computes the output of the network.\n",
    "\n",
    "Each layer consists of a linear part and a non-linear *activation* functions (except for the output layer, which usually don't use an activation function). There are many non-linear activation functions in Pytorch, check the [documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for a complete overview. A network is said to be *deep* if it has at least 3 layers (input, at least one hidden, and output).\n",
    "\n",
    "<center><a href=\"https://www.researchgate.net/publication/309592737_Classification_of_VoIP_and_non-VoIP_traffic_using_machine_learning_approaches\">\n",
    "    <img src=\"https://www.researchgate.net/profile/Mouhammd-Alkasassbeh/publication/309592737/figure/fig2/AS:423712664100865@1478032379613/MultiLayer-Perceptron-MLP-sturcture-334-MultiLayer-Perceptron-Classifier-MultiLayer.png\"></a></center>\n",
    "\n",
    "**Note**: Since an MLP manipulates vectors (= 1D-tensors) as inputs, in image processing we first have to transform our images into vectors. For instance, if each image is a tensor of size `[1, 28, 28]`, then we have to reshape it into a tensor of size `[1x28x28] = [784]`.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "First, we will write a succession of operations which correspond to applying the MLP classifier on the example batch `image_batch_example`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: reshape the image_batch_example into a batch of vectors (vectorized images):\n",
    "# 'image_batch_example' has size [batch_size, 1, 28, 28]\n",
    "# 'vectorized_batch' should have size [batch_size, 1*28*28]\n",
    "  \n",
    "vectorized_batch = image_batch_example.reshape(image_batch_example.shape[0], -1)\n",
    "print(vectorized_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create the input layer (linear and activation) and we pass the vectorized batch to it\n",
    "input_size = vectorized_batch.shape[-1]\n",
    "hidden_size = 10\n",
    "input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.Sigmoid())\n",
    "y = input_layer(vectorized_batch)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: create the two other layers (hidden_layer and output_layer):\n",
    "# - the hidden layer goes from 'hidden_size' to 'hidden_size', and uses a Sigmoid activation function\n",
    "#Â - the output layer goes from 'hidden_size' to 'output_size', which is the number of classes in the dataset (it uses no activation function)\n",
    "# - Compute z = hidden_layer(y) and out=output_layer(z). Print the size of 'out'.\n",
    "\n",
    "hidden_layer = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Sigmoid())\n",
    "output_size = num_classes\n",
    "output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "z = hidden_layer(y)\n",
    "out = output_layer(z) \n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we have done above, two important remarks can be made:\n",
    "\n",
    "- The output `out` has size `[batch_size, num_classes]` while the true labels `labels_batch_example` has size `[batch_size]`. This is because `out` contains a predicted probability for each class, while `labels_batch_example` simply contains the true labels.\n",
    "- In classification tasks, we want to output probabilities per class. However, nothing ensures that `out` corresponds to probabilities, since it is not normalized and we didn't use any output activation function (values can be negative and not sum up to 1).\n",
    "\n",
    "However, when training a classification network, we generally use the [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function, which alleviates these two issues. This loss is optimized for handling true labels instead of true probabilities per class, so you don't have to worry about it. Besides, it will automatically apply a [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) non-linearity to the predicted outputs, in order to normalize them as probabilities per class.\n",
    "\n",
    "**Note**: Instead of Cross Entropy, you can use the [Negative log-likelihood](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) similarly. It will also solve the first problem, but then you need to manually add a [log Softmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax) as output activation to normalize the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Cross Entropy as loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the error between the predicted labels 'out' and true labels 'labels_batch_example'\n",
    "loss_batch = loss_fn(out, labels_batch_example)\n",
    "print(loss_batch.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General module\n",
    "\n",
    "Now, let's create a general MLP classification network. It's a python class that inherits from the general `nn.Module` object, and it should contain at least 2 methods:\n",
    "\n",
    "- `__init__`, which initializes the network when instanciated (creates all the layers and stores some useful parameters if needed).\n",
    "- `forward`, which applies the forward pass (i.e., compute the output 'out' from the input and using the layers).\n",
    "\n",
    "You can add other methods if needed but these two are sufficient for now.\n",
    "\n",
    "**Note**: Remember that Python classes usually define and use some variables/data/tensors/dictionary etc. internally (this includes network layers) called *attributes*: they should be defined in the `__init__` method with a specific structure (the name should start by `self.`, as we did in lab 1). This allows you to access these attributes in other methods, or after defining your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, act_fn):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        \n",
    "        # TO DO: define the input, hidden, and output layers as before\n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), act_fn)\n",
    "        self.hidden_layer = nn.Sequential(nn.Linear(hidden_size, hidden_size), act_fn)\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # TO DO: write the 'forward' method, which computes the output 'out' from the input x\n",
    "        # It should apply sequentially the input, hidden, and output layer, as we did in the example before.\n",
    "        y = self.input_layer(x)\n",
    "        z = self.hidden_layer(y)\n",
    "        out = self.output_layer(z)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To DO: Instanciate an MLP classifier with a hidden size of 10 and a Sigmoid activation function\n",
    "input_size = train_data[0][0][0].shape[0]*train_data[0][0][0].shape[1]\n",
    "\n",
    "hidden_size = 10\n",
    "act_fn = nn.Sigmoid()\n",
    "output_size = num_classes\n",
    "\n",
    "model = MLPClassif(input_size, hidden_size, output_size, act_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization (ensure reproducibility: everybody should have the same results)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save / load the model's parameters as follow:\n",
    "torch.save(model.state_dict(), 'model_mlp_classif.pt')\n",
    "model.load_state_dict(torch.load('model_mlp_classif.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful command to get the total number of parameters in the model\n",
    "print('Total number of parameters:', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q1**</span> How many parameters are in the network?\n",
    "\n",
    "## Training\n",
    "\n",
    "We now write the function for training the network. It's very similar to what we did in lab 2, except now we process batches of data instead of the whole dataset at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_mlp_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate, verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            # TO DO: write the training procedure for each batch. This should consist of:\n",
    "            # - vectorizing the images (size should be (batch_size, input_size))\n",
    "            # - apply the forward pass (calculate the predicted labels from the vectorized images)\n",
    "            # - use the 'backward' method to compute the gradients\n",
    "            # - apply the gradient descent algorithm\n",
    "            # Also think of updating the loss at the current epoch\n",
    "            \n",
    "            # Prepare the inputs and labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            \n",
    "            # Forward pass\n",
    "            labels_predicted = model_tr(images)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = loss_fn(labels_predicted, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the batch loss to the current epoch loss\n",
    "            loss_current_epoch += loss.item()\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches\n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "        \n",
    "    return model_tr, loss_all_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define the training parameters and train the model\n",
    "# - 30 epochs\n",
    "#Â - learning rate = 0.01\n",
    "# - loss function: Cross Entropy\n",
    "# After training, save the model parameters and display the loss over epochs\n",
    "\n",
    "num_epochs = 30\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model, loss_all_epochs = training_mlp_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate)\n",
    "torch.save(model.state_dict(), 'model_mlp_classif.pt')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(torch.arange(num_epochs)+1, loss_all_epochs)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q2**</span> Put the plot above (loss over epochs) in your report. Do you remark something particular?\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Now the model is trained, we can evaluate it on the test dataset. We do that by predicting the labels using our model, and comparing it with the true labels. This allows us to compute the classification accuracy, which is provided in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: similar to the training loop, except we don't need to compute any gradient / backprop\n",
    "\n",
    "def eval_mlp_classifier(model, eval_dataloader):\n",
    "    \n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Evaluate the model on the test set:\n",
    "# - Instanciate an MLP newtork and load the trained parameters\n",
    "# - Apply the evaluation function using the test dataloader\n",
    "# - Print the test accuracy\n",
    "\n",
    "model = MLPClassif(input_size, hidden_size, num_classes, act_fn)\n",
    "model.load_state_dict(torch.load('model_mlp_classif.pt'))\n",
    "accuracy = eval_mlp_classifier(model, test_dataloader)\n",
    "print('Accuracy of the network on the test images:', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q3**</span> What is the accuracy on the test set? Given your answer at Q2, how could you improve it?\n",
    "\n",
    "## Influence of the activation function\n",
    "\n",
    "We used the MLP classifier with a Sigmoid activation function, but another common choice is the [Rectified Linear Unit](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU). Here we investigate how it performs compared to the Sigmoid-based network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Define an MLP classifier using a ReLU activation. \n",
    "model_relu = MLPClassif(input_size, hidden_size, num_classes, nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights (for reproducibility)\n",
    "torch.manual_seed(0)\n",
    "model_relu.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Train the network and evaluate it.\n",
    "\n",
    "# Training, plot the loss and save the model's parameters\n",
    "model_relu, loss_all_epochs_relu = training_mlp_classifier(model_relu, train_dataloader, num_epochs, loss_fn, learning_rate)\n",
    "torch.save(model_relu.state_dict(), 'model_mlp_classif_relu.pt')\n",
    "plt.figure()\n",
    "plt.plot(loss_all_epochs_relu)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the test set and print accuracy\n",
    "accuracy = eval_mlp_classifier(model_relu, test_dataloader)\n",
    "print('Accuracy of the network on the test images:', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q4**</span> What is the accuracy on the test set with this network? Which one (Sigmoid or ReLU) would you recommend to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the model capacity (bonus)\n",
    "\n",
    "In order to improve performance, a straightforward approach is to increase the model capacity, i.e., increase the number of parameters. There are basically two ways to do so: either increase the number of neurons in each layer (*width*) of increase the total number of layers (*depth*). Let's focus here on width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: define, train and evalute an MLP classifier model with a variable hidden_size:\n",
    "# - hidden_size should range in [3, 5, 10, 50, 100]\n",
    "# - the networks should use ReLU activation, and be trained for 30 epochs\n",
    "# - for each hidden size, print the number of parameters and the test accuracy\n",
    "# Remember to initialize the weights of the network after instanciating it for reproducibility.\n",
    "\n",
    "hidden_size_list = [3, 5, 10, 50, 100]\n",
    "for hidden_size in hidden_size_list:\n",
    "    model = MLPClassif(input_size, hidden_size, num_classes, nn.ReLU())\n",
    "    torch.manual_seed(0)\n",
    "    model.apply(init_weights)\n",
    "    model, _ = training_mlp_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate, verbose=True)\n",
    "    accuracy = eval_mlp_classifier(model, test_dataloader)\n",
    "    print()\n",
    "    print('Hidden size=', hidden_size, '--Total number of parameters:', sum(p.numel() for p in model.parameters()),\n",
    "          ' -- Test accuracy:', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Q5**</span> Report the test accuracy as a function of `hidden_size`. Which value of `hidden_size` would you use and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
